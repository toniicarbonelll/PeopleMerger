{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fda625d",
   "metadata": {},
   "source": [
    "**IMG 2 TEXT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e236d2e",
   "metadata": {},
   "source": [
    "This notebook aims at building the necessary functions to get the description of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e770b55",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "294c3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.join(os.getcwd(), '../'))\n",
    "from credentials import Credentials\n",
    "credentials = Credentials()\n",
    "os.environ[\"http_proxy\"] = credentials.http_proxy\n",
    "os.environ[\"https_proxy\"] = credentials.https_proxy\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8465e",
   "metadata": {},
   "source": [
    "# 1. BLIP-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea98d4",
   "metadata": {},
   "source": [
    "We are going to use the BLIP model with pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e1d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9606599",
   "metadata": {},
   "source": [
    "Loading the processor and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4d9616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m MODEL_ID = \u001b[33m\"\u001b[39m\u001b[33mSalesforce/blip2-flan-t5-xl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load model + processor\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m processor = \u001b[43mBlip2Processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model = Blip2ForConditionalGeneration.from_pretrained(\n\u001b[32m      7\u001b[39m     MODEL_ID,\n\u001b[32m      8\u001b[39m     torch_dtype=torch.float16,\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\transformers\\processing_utils.py:1394\u001b[39m, in \u001b[36mProcessorMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1392\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m args = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1395\u001b[39m processor_dict, kwargs = \u001b[38;5;28mcls\u001b[39m.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_args_and_dict(args, processor_dict, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\transformers\\processing_utils.py:1453\u001b[39m, in \u001b[36mProcessorMixin._get_arguments_from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1451\u001b[39m         attribute_class = \u001b[38;5;28mcls\u001b[39m.get_possibly_dynamic_module(class_name)\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m     args.append(\u001b[43mattribute_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1140\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1138\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1139\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2038\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2034\u001b[39m             vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m   2035\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2036\u001b[39m             )\n\u001b[32m   2037\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2038\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2045\u001b[39m         template = template.removesuffix(\u001b[33m\"\u001b[39m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2046\u001b[39m         vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\transformers\\utils\\hub.py:169\u001b[39m, in \u001b[36mlist_repo_templates\u001b[39m\u001b[34m(repo_id, local_files_only, revision, cache_dir, token)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveprefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3191\u001b[39m, in \u001b[36mHfApi.list_repo_tree\u001b[39m\u001b[34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[39m\n\u001b[32m   3189\u001b[39m encoded_path_in_repo = \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + quote(path_in_repo, safe=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m path_in_repo \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3190\u001b[39m tree_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mencoded_path_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3191\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaginate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtree_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursive\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpand\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3192\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mRepoFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRepoFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\huggingface_hub\\utils\\_pagination.py:36\u001b[39m, in \u001b[36mpaginate\u001b[39m\u001b[34m(path, params, headers)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fetch a list of models/datasets/spaces and paginate through results.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03mThis is using the same \"Link\" header format as GitHub.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[33;03m- https://docs.github.com/en/rest/guides/traversing-with-pagination#link-header\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m session = get_session()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m r = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m hf_raise_for_status(r)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\requests\\sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:95\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     97\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\http\\client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\A200348545\\AppData\\Local\\anaconda3\\envs\\bannerg\\Lib\\ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Choose the BLIP-2 model (Flan-T5-XL is a great starting point)\n",
    "MODEL_ID = \"Salesforce/blip2-flan-t5-xl\"\n",
    "\n",
    "# Load model + processor\n",
    "processor = Blip2Processor.from_pretrained(MODEL_ID)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c0647",
   "metadata": {},
   "source": [
    "Placing them to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee4da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b68e3b",
   "metadata": {},
   "source": [
    "Describing an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Image description: Describe only what is visually present in this image. It is a fashion campaign.\"\n",
    "    \"Do NOT describe the instructions or the task. \"\n",
    "    \"Write 6â€“8 vivid, detailed sentences about the objects, people, colors, environment, and scene.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa3394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of photos from an image of a collection of photos of people, people and objects, forming an image\n"
     ]
    }
   ],
   "source": [
    "# Load your image\n",
    "image = Image.open(\"../data/lacoste.png\").convert(\"RGB\")\n",
    "\n",
    "# Create inputs\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=350,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.8\n",
    ")\n",
    "\n",
    "caption = processor.decode(generated[0], skip_special_tokens=True)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee27cf",
   "metadata": {},
   "source": [
    "## 1.2 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559ba97",
   "metadata": {},
   "source": [
    "BLIP was perhaps created for detailed summarized descriptions of an image, we need a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab5d56",
   "metadata": {},
   "source": [
    "# 2. LLaVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2dd9d8",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131728ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ffadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "#processor\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90a848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54af16a6a2584c79a6d1051aa843b829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    quantization_config=quantization_config,  # <-- important\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee233d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b0d4711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d477f17",
   "metadata": {},
   "source": [
    "## Prompt 1: What is shown in this image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc1dfa",
   "metadata": {},
   "source": [
    "Duration:\n",
    "- 100 tokens --> 1.5 mins (better to do 1.5 mins multiple times lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4540769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "What is shown in this image? [/INST] The image shows a woman standing in front of a colorful, patterned backdrop. She is wearing a white blazer over a floral top and has long blonde hair. The backdrop has a patriotic theme, featuring what appears to be the American flag. The text on the image reads \"POLO Ralph Lauren\" and \"SUMMER STARTS HERE SHOP NOW,\" suggesting that this is an advertisement for the summer collection of the Ralph Lauren brand\n"
     ]
    }
   ],
   "source": [
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "image = Image.open(\"../data/polo.jpg\").convert(\"RGB\")\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06875b1d",
   "metadata": {},
   "source": [
    "## Prompt 2: Describe this image from a marketing perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466300f4",
   "metadata": {},
   "source": [
    "TIMES:\n",
    "- 100 tokens -> 1.5mins\n",
    "- 200 tokens -> 2mins!\n",
    "- 300 tokens -> 2mins 47s!\n",
    "- 500 tokens -> 3mins 47s! --> this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b9b9218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "Explain this image from a marketing perspective: [/INST] This image is a promotional advertisement for Polo Ralph Lauren's Summer collection. From a marketing perspective, the image is designed to showcase the brand's style and appeal to potential customers. Here are some key elements that contribute to the effectiveness of this advertisement:\n",
      "\n",
      "1. **Brand Identity**: The Polo logo is prominently displayed at the top, reinforcing brand recognition. The use of the brand name in a large, bold font also helps to establish a strong brand presence.\n",
      "\n",
      "2. **Seasonal Appeal**: The text \"SUMMER STARTS HERE\" suggests that the image is part of a seasonal campaign, aiming to capture the attention of consumers who are looking for summer fashion.\n",
      "\n",
      "3. **Product Promotion**: The model is wearing a white blazer, which is likely a key piece from the summer collection. By showcasing this garment, the advertisement is promoting the product to potential buyers.\n",
      "\n",
      "4. **Lifestyle Imagery**: The background features a cityscape, which is likely chosen to evoke a sense of urban style and sophistication. This kind of imagery is often used in fashion advertising to create a desirable lifestyle association with the brand.\n",
      "\n",
      "5. **Call to Action**: The phrase \"SHOP NOW\" is a clear call to action, encouraging viewers to visit the brand's website or stores to purchase the featured items.\n",
      "\n",
      "6. **Color Scheme**: The use of a color palette that includes pastel tones and the contrast between the white blazer and the colorful background helps to create a visually appealing and summery atmosphere.\n",
      "\n",
      "7. **Model Selection**: The model is chosen to represent the target demographic of the brand's customers. Her appearance, style, and pose are all carefully chosen to convey the brand's image and the appeal of the clothing.\n",
      "\n",
      "Overall, the image is designed to attract the attention of potential customers, showcase the brand's summer collection, and encourage them to make a purchase. \n"
     ]
    }
   ],
   "source": [
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "image = Image.open(\"../data/polo.jpg\").convert(\"RGB\")\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"Explain this image from a marketing perspective:\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38a7d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "Explain this image from a marketing perspective: [/INST] This image appears to be a promotional advertisement for Lacoste, a French sportswear brand. The ad features two individuals standing on a train platform, dressed in Lacoste's signature white polo shirts and white pants, with the Lacoste logo prominently displayed. The setting suggests a moment of leisure or travel, which aligns with the brand's image of sporty elegance and lifestyle.\n",
      "\n",
      "From a marketing perspective, the ad is designed to showcase the brand's clothing in a real-life context, emphasizing the versatility and style of the Lacoste polo shirts and pants. The use of two models, one male and one female, suggests that the brand's clothing is unisex and appeals to a wide range of consumers.\n",
      "\n",
      "The choice of location, a train platform, could be symbolic of the brand's association with sport and travel, as well as the idea of being on the move or ready for any occasion. The text \"Les Ardentes Virtuoses\" and \"Harlem 125th Street New York City\" might be referencing a specific collection or campaign, possibly related to a collaboration with a music artist or a themed collection inspired by New York City.\n",
      "\n",
      "Overall, the ad is designed to evoke a sense of style, sportiness, and travel, while also promoting the brand's clothing and its association with a particular lifestyle. \n"
     ]
    }
   ],
   "source": [
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "image = Image.open(\"../data/lacoste.png\").convert(\"RGB\")\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"Explain this image from a marketing perspective:\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "718bd14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "Explain this image from a marketing perspective: [/INST] This image appears to be an advertisement for a clothing brand. From a marketing perspective, the following elements are noteworthy:\n",
      "\n",
      "1. **Visual Appeal**: The model is dressed in a chic, oversized trench coat, which is a fashionable choice. The coat's length and the model's pose suggest a sense of style and sophistication.\n",
      "\n",
      "2. **Message**: The text \"THIS TRENCH IS 30 YEARS OLD\" is a play on words, implying that the trench coat is a classic, timeless piece. The phrase \"buy clothes that will last longer than the trends\" aligns with the brand's message, suggesting that investing in quality, long-lasting clothing is a better choice than following fleeting fashion trends.\n",
      "\n",
      "3. **Branding**: The text \"BE OLDER.\" is a call to action that encourages the viewer to consider the value of age and experience in their clothing choices. This phrase is likely to resonate with consumers who value quality and durability over fast fashion.\n",
      "\n",
      "4. **Color Scheme**: The use of a neutral color palette, with the model's outfit and the background, creates a clean and sophisticated look that is likely to appeal to a wide range of consumers.\n",
      "\n",
      "5. **Target Audience**: The advertisement seems to target consumers who are interested in fashion and are looking for quality, long-lasting clothing. The message suggests that the brand's target demographic might be older individuals who value experience and quality over trendiness.\n",
      "\n",
      "6. **Emotional Connection**: The model's serious expression and the overall composition of the image suggest a sense of elegance and sophistication, which can evoke an emotional connection with the viewer.\n",
      "\n",
      "Overall, the advertisement is designed to appeal to consumers who are looking for quality, long-lasting clothing and are interested in fashion that transcends trends. \n"
     ]
    }
   ],
   "source": [
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "image = Image.open(\"../data/trench.png\").convert(\"RGB\")\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"Explain this image from a marketing perspective:\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15a31e",
   "metadata": {},
   "source": [
    "## Prompt 3: Analyze elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd71cd6",
   "metadata": {},
   "source": [
    "### image 1 (innovative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11e1dfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "This is a fashion banner, it has an image and probably some background and text. Focus only on the image and answer very briefly:\n",
      "1. short answer: where is the scene and what is in the background?\n",
      "2. short answer: how many people are in the image? where are they placed and how are they interacting with the scenario? which shot (medium, long...)?\n",
      "3. short answer: what is their fashion style\n",
      "4. short answer: what are the colors and emotions of this image? [/INST] 1. The scene is set against a black metal fence.\n",
      "2. There are two people in the image. They are standing in front of the fence, leaning against it. The shot appears to be a medium shot, capturing them from the waist up.\n",
      "3. The fashion style of the two people is casual and colorful, with one person wearing a pink jacket and the other in a yellow jacket.\n",
      "4. The colors in the image are bright and vibrant, with the pink and yellow jackets standing out against the black fence. The emotions conveyed by the image are energetic and youthful. \n"
     ]
    }
   ],
   "source": [
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "image = Image.open(\"../data/innovative.png\").convert(\"RGB\")\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"This is a fashion banner, it has an image and probably some background and text. Focus only on the image and answer very briefly:\\n\"\n",
    "                    \"1. short answer: where is the scene and what is in the background?\\n\"\n",
    "                    \"2. short answer: how many people are in the image? where are they placed and how are they interacting with the scenario? which shot (medium, long...)?\\n\"\n",
    "                    \"3. short answer: what is their fashion style\\n\"\n",
    "                    \"4. short answer: what are the colors and emotions of this image?\"\n",
    "                )\n",
    "            },\n",
    "            {\"type\": \"image\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a1aca",
   "metadata": {},
   "source": [
    "### image 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e38b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "This is a fashion banner, it has an image and probably some background and text. Focus only on the image and answer very briefly:\n",
      "1. short answer: where is the scene and what is in the background?\n",
      "2. short answer: how many people are in the image? where are they placed and how are they interacting with the scenario? which shot (medium, long...)?\n",
      "3. short answer: what is their fashion style\n",
      "4. short answer: what are the colors and emotions of this image? [/INST] 1. The scene appears to be a studio setting, with a plain white background.\n",
      "2. There is one person in the image, a woman. She is standing in the center of the frame, looking directly at the camera. She is wearing a long coat and has her head covered with the coat's collar.\n",
      "3. The fashion style of the image is minimalist and modern, with a focus on the coat as the main piece of clothing.\n",
      "4. The colors in the image are neutral, with the woman's coat being the main focal point. The emotions conveyed by the image are likely to be serious or contemplative, given the woman's expression and the way she is dressed. \n"
     ]
    }
   ],
   "source": [
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "image = Image.open(\"../data/trench.png\").convert(\"RGB\")\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"This is a fashion banner, it has an image and probably some background and text. Focus only on the image and answer very briefly:\\n\"\n",
    "                    \"1. short answer: where is the scene and what is in the background?\\n\"\n",
    "                    \"2. short answer: how many people are in the image? where are they placed and how are they interacting with the scenario? which shot (medium, long...)?\\n\"\n",
    "                    \"3. short answer: what is their fashion style\\n\"\n",
    "                    \"4. short answer: what are the colors and emotions of this image?\"\n",
    "                )\n",
    "            },\n",
    "            {\"type\": \"image\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c1119",
   "metadata": {},
   "source": [
    "### image 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00a708a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "This is a fashion banner, it has an image and probably some background and text. Focus only on the image and answer very briefly:\n",
      "1. short answer: where is the scene and what is in the background?\n",
      "2. short answer: how many people are in the image? where are they placed and how are they interacting with the scenario? which shot (medium, long...)?\n",
      "3. short answer: what is their fashion style\n",
      "4. short answer: what are the colors and emotions of this image? [/INST] 1. The scene is set in a natural outdoor environment, possibly a park or garden, with trees and a bench in the background.\n",
      "2. There are two people in the image. They are placed on opposite sides of the bench, with one person sitting on the bench and the other standing behind it. They are interacting with the scenario by posing for the photo. The shot appears to be a medium shot, focusing on the upper body and the lower body of the individuals.\n",
      "3. The fashion style of the individuals is formal and elegant, with the person sitting wearing a long dress and the person standing wearing a suit.\n",
      "4. The colors in the image are muted, with a focus on earth tones and natural lighting. The emotions conveyed by the image are serene and sophisticated. \n"
     ]
    }
   ],
   "source": [
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "image = Image.open(\"../data/10magazine.jpg\").convert(\"RGB\")\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"This is a fashion banner, it has an image and probably some background and text. Focus only on the image and answer very briefly:\\n\"\n",
    "                    \"1. short answer: where is the scene and what is in the background?\\n\"\n",
    "                    \"2. short answer: how many people are in the image? where are they placed and how are they interacting with the scenario? which shot (medium, long...)?\\n\"\n",
    "                    \"3. short answer: what is their fashion style\\n\"\n",
    "                    \"4. short answer: what are the colors and emotions of this image?\"\n",
    "                )\n",
    "            },\n",
    "            {\"type\": \"image\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3f692",
   "metadata": {},
   "source": [
    "## Prompt 4: Reverse prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dddb7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "Give me a prompt to generate this image in about 75 words [/INST] \"Create an image featuring two individuals standing in front of a black metal fence. The person on the left has a vibrant pink jacket and yellow pants, while the person on the right is wearing a pink hoodie and white pants. Both are looking directly at the camera with a serious expression. Above them, superimpose the text 'Revolutionizing Fashion Marketing' in bold, white font. Below them, add the text 'Innovative campaigns that changed the game' in a smaller\n"
     ]
    }
   ],
   "source": [
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "image = Image.open(\"../data/innovative.png\").convert(\"RGB\")\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"Give me a prompt to generate this image in about 75 words\"\n",
    "                )\n",
    "            },\n",
    "            {\"type\": \"image\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a436fff",
   "metadata": {},
   "source": [
    "# FINAL: LLAVA x PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2182dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2681b71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "#processor\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6557d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140cc5b2d4a4467facf42b1541b08e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    quantization_config=quantization_config,  # <-- important\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbda36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30379de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568b558",
   "metadata": {},
   "source": [
    "Now the idea will be to use this prompt (see below) for all the images that we have in /data, and to store the output information in a dictionary. The idea will be to merge the different point of images to create new ones using AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84afc32e",
   "metadata": {},
   "source": [
    "Let's retrieve first the list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a5ef202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beach.jpg',\n",
       " 'bixby.jpg',\n",
       " 'dior.png',\n",
       " 'gucci.jpg',\n",
       " 'loewe.jpg',\n",
       " 'orange.jpg',\n",
       " 'outdoor.jpg',\n",
       " 'paris.jpg',\n",
       " 'polo.jpg',\n",
       " 'polo2.jpg',\n",
       " 'street.jpg',\n",
       " 'trench.png']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = [file for file in os.listdir('../data') if \".\" in file] #retrieving only the images\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2bf13",
   "metadata": {},
   "source": [
    "This is the function we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4b16c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2text(image_src:str) -> str:\n",
    "    \"\"\"Turns images into text (fixed prompt for now)\n",
    "\n",
    "    Args:\n",
    "        image_src (str): Source of the image\n",
    "    \n",
    "    Returns:\n",
    "        (str): Decoded output\n",
    "    \"\"\"\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(2)\n",
    "    print(\"Cache is empty now!\")\n",
    "\n",
    "    # FIXED PROMPT\n",
    "    conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    \"This is an image for a fashion ad. Ignore the text on the foto. Focus only on the image:\\n\"\n",
    "                    \"1. Who is in the image? Describe their physical appearance (not their clothes): gender, race (important), hairstyle...\\n\"\n",
    "                    '2. What clothes are they wearing and which is their fashion style (use word \"person\")?\\n'\n",
    "                    '3. Where is the scene and what is in the background? How is the person posing relative to the scene? (use word \"person\")'                )\n",
    "            },\n",
    "            {\"type\": \"image\"}\n",
    "        ],\n",
    "    },\n",
    "    ]\n",
    "\n",
    "    # prepare image and text prompt, using the appropriate prompt template\n",
    "    try:\n",
    "        image = Image.open(image_src).convert(\"RGB\")\n",
    "    \n",
    "    except:\n",
    "        print(\"Unable to open image...\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Processing image {image_src}...\")\n",
    "        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # autoregressively complete prompt\n",
    "        output = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "        return (processor.decode(output[0], skip_special_tokens=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efbbac",
   "metadata": {},
   "source": [
    "Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acfb849e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m output = \u001b[43mimg2text\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m output\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mimg2text\u001b[39m\u001b[34m(image_src)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Turns images into text (fixed prompt for now)\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03m    (str): Decoded output\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m torch.cuda.empty_cache()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCache is empty now!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# FIXED PROMPT\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "output = img2text('../data/'+images[3])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "750485b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The image features a person who appears to be a woman. She has dark skin and her hair is styled in a short, natural cut.',\n",
       " 'The person is wearing a black and white checkered dress with a high collar and a bow tie at the neck. The dress has a full skirt and long sleeves. The fashion style of the person can be described as a blend of classic and modern, with a touch of vintage inspiration.',\n",
       " 'The scene is set against a plain, light-colored background. The person is posing in a dynamic and confident manner, with one hand on their hip and the other extended outward. The pose suggests movement and energy, which is in line with the fashion style being showcased. The person is the central focus of the image, and their position relative to the background emphasizes their outfit and the overall composition of the advertisement.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output = output.split(\"[/INST]\")[1] #getting the output\n",
    "final_output = [line.strip()[3:] for line in final_output.split(\"\\n\") if len(line)>1] #splitting by lines and removing the \"1.\" from the start\n",
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99634420",
   "metadata": {},
   "source": [
    "Now let's generate output for all the images and store them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5daaf2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/beach.jpg...\n",
      "['The image features a person who appears to be a woman. She has dark hair, and her skin tone is light. She is wearing a yellow hoodie and yellow sweatpants.', 'The person is wearing a casual, sporty fashion style, characterized by the matching yellow hoodie and sweatpants.', 'The scene is set outdoors, with a basketball hoop in the background. The person is posing in front of the hoop, with her hands on her hips and her head turned to the side, giving a confident and stylish appearance.']\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/bixby.jpg...\n",
      "['The image features a male model. He has short hair and appears to be of African descent.', 'The model is wearing a white suit with a white shirt and a white bow tie. His fashion style can be described as classic and formal.', 'The scene is set in a room with a vintage or classic aesthetic. In the background, there are framed pictures on the wall, including one that appears to be a painting of a nude woman. The model is posing in front of the wall, standing between two chairs with a relaxed yet poised posture. He is looking directly at the camera, which suggests a confident and polished demeanor.']\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/dior.png...\n",
      "['The image features a person who appears to be a woman. She has blonde hair and is wearing makeup that includes dark lipstick.', 'The person is wearing a black dress with a high neckline and a flared skirt. The dress has a textured pattern and is adorned with what seems to be a floral or paisley design. The person is also wearing black high heels.', 'The scene is set against a dark gray background. The person is posing in a dynamic manner, with one leg lifted and the other bent at the knee, giving the impression of movement. The person is holding a small, colorful clutch in their right hand, which is raised slightly above their hip. The pose and the way the person is holding the clutch suggest a sense of elegance and style.']\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/gucci.jpg...\n",
      "['The image features a person who appears to be a woman. She has dark skin and is wearing her hair in an updo.', 'The person is wearing a black and white checkered dress with a high collar and a bow tie at the neck. The dress has a full skirt and is paired with black high-heeled shoes. The overall fashion style can be described as a blend of classic and modern elements, with a touch of vintage inspiration.', \"The scene is set against a plain, light-colored background. The person is posing in a dynamic and confident manner, with one hand on her hip and the other extended outward. Her pose suggests movement and energy, which is in line with the fashion style being showcased. The person is the central focus of the image, and her pose and attire are the main elements that draw the viewer's attention.\"]\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/loewe.jpg...\n",
      "['The image features a male model. He has dark hair and a beard, and appears to be of Caucasian descent.', 'The model is wearing a brown leather jacket and black pants. He is also holding a brown bag.', 'The scene is set against a white brick wall with a decorative element that resembles a heart shape. The model is posing on a stool, sitting with one leg crossed over the other, and holding the bag in his lap. He is looking directly at the camera with a neutral expression. The overall fashion style of the image suggests a casual yet stylish look, possibly for a brand that focuses on leather accessories and casual outerwear.']\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/orange.jpg...\n",
      "['The image features a person who appears to be a young adult female. She has dark skin and her hair is styled in a way that it falls over her shoulders.', 'The person is wearing a bright orange hoodie and matching orange socks. Her fashion style can be described as casual and sporty, with a focus on bold, vibrant colors.', \"The scene takes place in a room with orange walls, which complements the person's outfit. The person is sitting on a large, round, orange cushion or ottoman, which is placed against the wall. She is posing in a relaxed manner, with one leg crossed over the other, and her hands resting on her knees. The person's pose and the choice of the orange background create a cohesive and visually appealing image that highlights the clothing and the color scheme.\"]\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/outdoor.jpg...\n",
      "['In the image, there is a person who appears to be a woman. She has dark skin and her hair is styled in an afro.', 'The person is wearing a bright orange dress. The fashion style of the person can be described as summery and vibrant, with a focus on bold colors and light fabrics.', 'The scene takes place outdoors, with a large rock formation in the background. The person is posing on top of the rock formation, leaning against it with one hand. The pose gives the impression that the person is enjoying a sunny day outdoors.']\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/paris.jpg...\n",
      "['The image features a person who appears to be a woman. She has light skin and is wearing makeup that includes dark lipstick and eye makeup. Her hairstyle is a short, blonde bob.', 'The person is wearing a black dress with a lace or embroidered detail on the bodice. The dress has a flared skirt and is paired with black lace gloves. The overall fashion style of the person can be described as elegant and sophisticated.', \"The scene takes place in a room with a brick wall in the background. The person is posing in front of a mirror, which reflects their image. They are standing with their hands clasped together in front of them, and their pose suggests a sense of confidence and poise. The person's reflection in the mirror creates a sense of depth and symmetry in the image.\"]\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/polo.jpg...\n",
      "['The image features a person who appears to be a young woman with long blonde hair. She has a fair complexion and is wearing makeup that includes lipstick and eye makeup.', 'The person is wearing a white blazer over a floral top, paired with white pants. The fashion style can be described as chic and casual, with a touch of femininity.', 'The scene is set outdoors, with a cityscape visible in the background. The person is posing in front of a large, colorful quilt that is draped over a structure, which adds a vibrant and artistic element to the image. The person is standing in front of the quilt, with the cityscape behind them, creating a contrast between the urban environment and the quilted fabric. The pose is relaxed yet poised, with the person looking directly at the camera.']\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/polo2.jpg...\n",
      "['The image features a person who appears to be a young adult male. He has short, light-colored hair and is wearing glasses.', 'The person is dressed in a suit and tie, with a pink shirt and a blue blazer. The fashion style can be described as formal or business casual.', 'The scene is set in a room with a couch and a window. The person is sitting on the couch, holding a book, and posing with one leg crossed over the other. The pose suggests a relaxed yet sophisticated demeanor. The background is blurred, but it appears to be a well-lit, comfortable interior space.']\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/street.jpg...\n",
      "['In the image, there is a person who appears to be a woman. She has dark hair, which could be black or dark brown, styled in a way that it falls over her shoulders. She has a neutral expression on her face.', 'The person is wearing a brown jumpsuit with wide-leg pants. The jumpsuit has a high neckline and long sleeves, which gives it a formal or elegant appearance. The fashion style of the person can be described as chic and sophisticated.', 'The scene is set on a city street, with a building featuring a window and a street sign in the background. The person is standing on the sidewalk, posing with one hand on her hip and the other hand resting on her thigh. She is looking directly at the camera, which gives the impression that she is the focal point of the image. The pose and the setting suggest a sense of urban style and confidence.']\n",
      "................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache is empty now!\n",
      "Processing image ../data/trench.png...\n",
      "['The image features a person who appears to be a woman. She has a fair complexion and her hair is styled in a way that it falls over her shoulders.', 'The person is wearing a long, oversized coat that reaches down to her ankles. The coat has a high collar and appears to be made of a heavy fabric, suggesting a fashion style that is both functional and fashionable.', \"The scene is set against a plain, light-colored background that provides a neutral backdrop for the person. The person is posing in a way that she is looking directly at the camera, with her head slightly tilted to one side. Her pose is elegant and poised, with her hands resting at her sides. The overall composition of the image suggests a focus on the coat and the person's style, with the background serving to highlight the clothing.\"]\n",
      "................\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_dict = dict() #global dictionary\n",
    "\n",
    "for image in images:\n",
    "    # getting the output\n",
    "    base_url = '../data/'\n",
    "    output = img2text(base_url+image) #using LLAVA\n",
    "    final_output = output.split(\"[/INST]\")[1] #getting the output\n",
    "    final_output = [line.strip()[3:] for line in final_output.split(\"\\n\") if len(line)>1] #splitting by lines and removing the \"1.\" from the start\n",
    "    print(final_output)\n",
    "    # filling the dictionary\n",
    "    img_description = dict() #specific dictionary for an image\n",
    "    img_description[\"person\"] = final_output[0]\n",
    "    img_description[\"clothes\"] = final_output[1]\n",
    "    img_description[\"scenario\"] = final_output[2]\n",
    "    images_dict[image] = img_description\n",
    "    #store the dictionary at every iteration just in case something happens\n",
    "    with open('images_dict.pkl', 'wb') as fp:\n",
    "        pickle.dump(images_dict, fp)\n",
    "    print(\"................\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2fba341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beach.jpg': {'person': 'The image features a person who appears to be a woman. She has dark hair, and her skin tone is light. She is wearing a yellow hoodie and yellow sweatpants.',\n",
       "  'clothes': 'The person is wearing a casual, sporty fashion style, characterized by the matching yellow hoodie and sweatpants.',\n",
       "  'scenario': 'The scene is set outdoors, with a basketball hoop in the background. The person is posing in front of the hoop, with her hands on her hips and her head turned to the side, giving a confident and stylish appearance.'},\n",
       " 'bixby.jpg': {'person': 'The image features a male model. He has short hair and appears to be of African descent.',\n",
       "  'clothes': 'The model is wearing a white suit with a white shirt and a white bow tie. His fashion style can be described as classic and formal.',\n",
       "  'scenario': 'The scene is set in a room with a vintage or classic aesthetic. In the background, there are framed pictures on the wall, including one that appears to be a painting of a nude woman. The model is posing in front of the wall, standing between two chairs with a relaxed yet poised posture. He is looking directly at the camera, which suggests a confident and polished demeanor.'},\n",
       " 'dior.png': {'person': 'The image features a person who appears to be a woman. She has blonde hair and is wearing makeup that includes dark lipstick.',\n",
       "  'clothes': 'The person is wearing a black dress with a high neckline and a flared skirt. The dress has a textured pattern and is adorned with what seems to be a floral or paisley design. The person is also wearing black high heels.',\n",
       "  'scenario': 'The scene is set against a dark gray background. The person is posing in a dynamic manner, with one leg lifted and the other bent at the knee, giving the impression of movement. The person is holding a small, colorful clutch in their right hand, which is raised slightly above their hip. The pose and the way the person is holding the clutch suggest a sense of elegance and style.'},\n",
       " 'gucci.jpg': {'person': 'The image features a person who appears to be a woman. She has dark skin and is wearing her hair in an updo.',\n",
       "  'clothes': 'The person is wearing a black and white checkered dress with a high collar and a bow tie at the neck. The dress has a full skirt and is paired with black high-heeled shoes. The overall fashion style can be described as a blend of classic and modern elements, with a touch of vintage inspiration.',\n",
       "  'scenario': \"The scene is set against a plain, light-colored background. The person is posing in a dynamic and confident manner, with one hand on her hip and the other extended outward. Her pose suggests movement and energy, which is in line with the fashion style being showcased. The person is the central focus of the image, and her pose and attire are the main elements that draw the viewer's attention.\"},\n",
       " 'loewe.jpg': {'person': 'The image features a male model. He has dark hair and a beard, and appears to be of Caucasian descent.',\n",
       "  'clothes': 'The model is wearing a brown leather jacket and black pants. He is also holding a brown bag.',\n",
       "  'scenario': 'The scene is set against a white brick wall with a decorative element that resembles a heart shape. The model is posing on a stool, sitting with one leg crossed over the other, and holding the bag in his lap. He is looking directly at the camera with a neutral expression. The overall fashion style of the image suggests a casual yet stylish look, possibly for a brand that focuses on leather accessories and casual outerwear.'},\n",
       " 'orange.jpg': {'person': 'The image features a person who appears to be a young adult female. She has dark skin and her hair is styled in a way that it falls over her shoulders.',\n",
       "  'clothes': 'The person is wearing a bright orange hoodie and matching orange socks. Her fashion style can be described as casual and sporty, with a focus on bold, vibrant colors.',\n",
       "  'scenario': \"The scene takes place in a room with orange walls, which complements the person's outfit. The person is sitting on a large, round, orange cushion or ottoman, which is placed against the wall. She is posing in a relaxed manner, with one leg crossed over the other, and her hands resting on her knees. The person's pose and the choice of the orange background create a cohesive and visually appealing image that highlights the clothing and the color scheme.\"},\n",
       " 'outdoor.jpg': {'person': 'In the image, there is a person who appears to be a woman. She has dark skin and her hair is styled in an afro.',\n",
       "  'clothes': 'The person is wearing a bright orange dress. The fashion style of the person can be described as summery and vibrant, with a focus on bold colors and light fabrics.',\n",
       "  'scenario': 'The scene takes place outdoors, with a large rock formation in the background. The person is posing on top of the rock formation, leaning against it with one hand. The pose gives the impression that the person is enjoying a sunny day outdoors.'},\n",
       " 'paris.jpg': {'person': 'The image features a person who appears to be a woman. She has light skin and is wearing makeup that includes dark lipstick and eye makeup. Her hairstyle is a short, blonde bob.',\n",
       "  'clothes': 'The person is wearing a black dress with a lace or embroidered detail on the bodice. The dress has a flared skirt and is paired with black lace gloves. The overall fashion style of the person can be described as elegant and sophisticated.',\n",
       "  'scenario': \"The scene takes place in a room with a brick wall in the background. The person is posing in front of a mirror, which reflects their image. They are standing with their hands clasped together in front of them, and their pose suggests a sense of confidence and poise. The person's reflection in the mirror creates a sense of depth and symmetry in the image.\"},\n",
       " 'polo.jpg': {'person': 'The image features a person who appears to be a young woman with long blonde hair. She has a fair complexion and is wearing makeup that includes lipstick and eye makeup.',\n",
       "  'clothes': 'The person is wearing a white blazer over a floral top, paired with white pants. The fashion style can be described as chic and casual, with a touch of femininity.',\n",
       "  'scenario': 'The scene is set outdoors, with a cityscape visible in the background. The person is posing in front of a large, colorful quilt that is draped over a structure, which adds a vibrant and artistic element to the image. The person is standing in front of the quilt, with the cityscape behind them, creating a contrast between the urban environment and the quilted fabric. The pose is relaxed yet poised, with the person looking directly at the camera.'},\n",
       " 'polo2.jpg': {'person': 'The image features a person who appears to be a young adult male. He has short, light-colored hair and is wearing glasses.',\n",
       "  'clothes': 'The person is dressed in a suit and tie, with a pink shirt and a blue blazer. The fashion style can be described as formal or business casual.',\n",
       "  'scenario': 'The scene is set in a room with a couch and a window. The person is sitting on the couch, holding a book, and posing with one leg crossed over the other. The pose suggests a relaxed yet sophisticated demeanor. The background is blurred, but it appears to be a well-lit, comfortable interior space.'},\n",
       " 'street.jpg': {'person': 'In the image, there is a person who appears to be a woman. She has dark hair, which could be black or dark brown, styled in a way that it falls over her shoulders. She has a neutral expression on her face.',\n",
       "  'clothes': 'The person is wearing a brown jumpsuit with wide-leg pants. The jumpsuit has a high neckline and long sleeves, which gives it a formal or elegant appearance. The fashion style of the person can be described as chic and sophisticated.',\n",
       "  'scenario': 'The scene is set on a city street, with a building featuring a window and a street sign in the background. The person is standing on the sidewalk, posing with one hand on her hip and the other hand resting on her thigh. She is looking directly at the camera, which gives the impression that she is the focal point of the image. The pose and the setting suggest a sense of urban style and confidence.'},\n",
       " 'trench.png': {'person': 'The image features a person who appears to be a woman. She has a fair complexion and her hair is styled in a way that it falls over her shoulders.',\n",
       "  'clothes': 'The person is wearing a long, oversized coat that reaches down to her ankles. The coat has a high collar and appears to be made of a heavy fabric, suggesting a fashion style that is both functional and fashionable.',\n",
       "  'scenario': \"The scene is set against a plain, light-colored background that provides a neutral backdrop for the person. The person is posing in a way that she is looking directly at the camera, with her head slightly tilted to one side. Her pose is elegant and poised, with her hands resting at her sides. The overall composition of the image suggests a focus on the coat and the person's style, with the background serving to highlight the clothing.\"}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('images_dict.pkl', 'rb') as fp:\n",
    "#     images_dict = pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bannerg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
